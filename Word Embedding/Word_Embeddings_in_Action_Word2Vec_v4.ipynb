{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWWH8Gvgd09_"
   },
   "source": [
    "# Word Embeddings in Action - Word2Vec\n",
    "\n",
    "### Word embeddings are a really useful way of converting text into a format that is interpretable to the model while still keeping it's semantic meaning intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPnpxm2s7Zty"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "AgII8L1Xd0-Q",
    "outputId": "6e3f6605-bfee-4605-d913-cdf30c5db1f1"
   },
   "outputs": [],
   "source": [
    "# Load the twitter dataset\n",
    "df = pd.read_csv('tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epUh25Opuw3H"
   },
   "source": [
    "Skip the code block below if you have already downloaded the stopwords before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download stopwords (one-time download)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FtVATpXrmVy2",
    "outputId": "f3600cd6-2c41-40b9-fd57-6a169ab48551"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "AuuF55_xnWYY",
    "outputId": "62b3962f-9827-410d-cc5a-70a787b8cd12"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet') #one-time download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yy-hraLZnJYC"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pENuV1ltlJeM"
   },
   "outputs": [],
   "source": [
    "# function for text pre-processing\n",
    "def tweet_cleaner(text):\n",
    "    newString=re.sub(r'@[A-Za-z0-9]+','',text)                     #removing user mentions\n",
    "    newString=re.sub(\"#\",\"\",newString)                             #removing hashtag symbol\n",
    "    newString= re.sub(r'http\\S+', '', newString)                   #removing links\n",
    "    newString= re.sub(r\"'s\\b\",\"\",newString)                        #removing 's\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", newString)             #Fetching out only letters\n",
    "    lower_case = letters_only.lower()                              #converting everything to lowercase\n",
    "    tokens = [w for w in lower_case.split() if not w in stop_words]#stopwords removal\n",
    "    newString=''\n",
    "    for i in tokens:                                                 \n",
    "        newString=newString+lemmatizer.lemmatize(i)+' '            #converting words to lemma                               \n",
    "    \n",
    "    return newString.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFPeit-wlQ83"
   },
   "outputs": [],
   "source": [
    "# empty list to store tweets after pre-processing\n",
    "cleaned_tweets = []\n",
    "\n",
    "# pre-processing the tweets\n",
    "for i in df['tweet']:\n",
    "  cleaned_tweets.append(tweet_cleaner(i))\n",
    "\n",
    "#creating new column  \n",
    "df['cleaned_tweets']= cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "nG9HS92gnjhQ",
    "outputId": "7bc3b556-3961-4c48-faab-9cd51013710e"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Fpt6-ned1AI"
   },
   "source": [
    "### Using Google's pre-trained Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "WQNZ7Exh3jky",
    "outputId": "d22cb1b5-0d68-46a9-d079-c52e84e1a0bd"
   },
   "outputs": [],
   "source": [
    "# download and extract word2vec embeddings \n",
    "! wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "! gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "GoHGUSuId1AO",
    "outputId": "1b0d7a89-ad6c-4d21-ffc7-e645a0e3614e"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# path of the downloaded model\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# load into gensim\n",
    "w2vec = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5WyFsVHd1Aj"
   },
   "source": [
    "Once you have executed the above code, your word2vec model is finally installed and loaded. Let's explore some of the features of this model.\n",
    "\n",
    "__Contextual Relationship Between Words__\n",
    "\n",
    " - One of the impressive things about word2vec is it's ability to capture semantic relationship between words. That is the reason that you can do cool stuff like perform linear algebra on words and get an appropriate output. Have a look at the following example:\n",
    "\n",
    "    `airplane - fly + drive = car`\n",
    "\n",
    " - If you pass the left hand side of the above equation to the model, it will give the right handside. Which makes sense because what would you get if you remove the ability to fly from an airplane? And add the ability to drive? You would get a car!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "assnsO6Ld1Ap"
   },
   "source": [
    "### Text Classification using Word2Vec\n",
    "\n",
    "Let's now get back to our task to classify our twitter data by using __word2vec__ as features. However, word2vec gives vector representation of individual words, in order to find the same for a statement or a document you can take mean of the vectors of it's constituent words.\n",
    "\n",
    "<br>\n",
    "\n",
    "Please note that the length of every vector of the pre-trained word2vec embeddings is 300.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGTv0F10qdhO"
   },
   "outputs": [],
   "source": [
    "# function to get vector representation of a tweet\n",
    "def word_vector(tokens):\n",
    "    vec = np.zeros((1,300))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2vec.wv.word_vec(word)\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "xV0yy-P0sPKf",
    "outputId": "a050f111-1041-463d-e379-e3e74281e408"
   },
   "outputs": [],
   "source": [
    "# empty array of shape (no. of tweets X 300) to store word2vec features\n",
    "wordvec_arrays = np.zeros((len(df), 300))\n",
    "\n",
    "for i,j in enumerate(df['cleaned_tweets']):\n",
    "  wordvec_arrays[i,:] = word_vector(j.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lebArT-0tIXg",
    "outputId": "6aa90c38-3b4d-45b0-fc8e-b6c4d8df6db9"
   },
   "outputs": [],
   "source": [
    "wordvec_arrays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t037dps1d1BL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into train and test\n",
    "y = df['label']\n",
    "X_train_wv, X_test_wv, y_train_wv, y_test_wv = train_test_split(wordvec_arrays, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLKwHmgtd1Bl"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "Tn6YmHjBd1B4",
    "outputId": "a5bc31ce-fab5-48e0-ff82-c76b627d30f3"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the train dataset\n",
    "model = model.fit(X_train_wv, y_train_wv)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "pred = model.predict(X_test_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1SkCDYjWd1CI",
    "outputId": "f055af63-88d6-4345-f0ff-c22a39519a36"
   },
   "outputs": [],
   "source": [
    "# check the accuracy of the model\n",
    "print(\"F1 Score:\", f1_score(y_test_wv, pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Word Embeddings in Action - Word2Vec_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
